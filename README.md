# è¿™æ˜¯ä¸€ä¸ªç„Šæ¥chatgpt/gpt3å’Œvitsçš„åç«¯apiç¨‹åº
ç”¨apiå¯åŠ¨çš„ä¸»è¦ç›®çš„æ˜¯æ­å»ºloveliveçš„èŠå¤©ç½‘ç«™ï¼Œå…¶å®16å·å°±å·²ç»æŠŠè¿™ä¸ªé¡¹ç›®æå¥½äº†ï¼Œåªæ˜¯å› ä¸ºloveliveä¸åƒé‚¦é‚¦å’Œå°‘æ­Œé‚£æ ·æœ‰live2dæ¨¡å‹ï¼Œæ‰¾å„ç§æ–¹æ¡ˆæç½®äº†ä¸€å‘¨å¤šï¼Œç›´åˆ°èŠ±é’±æ•´äº†ä¸ªæ¨¡å‹æƒ³èµ·æ¥()ï¼Œè€Œä¸”å‘ç°å¤§å®¶å¯¹è¿™ç§å½¢å¼çš„çƒ­æƒ…å¾ˆé«˜ï¼Œå°±æŠŠè¿™ä¸ªæ”¾åœ¨æ–‡ä»¶å¤¹é‡Œåƒç°çš„é¡¹ç›®æ‹¿å‡ºæ¥äº†ã€‚
å‚è€ƒç”¨é“¾æ¥http://43.159.36.6:8080/
é¡¹ç›®åœ°å€https://drive.google.com/drive/folders/1vtootVMQ7wTOQwd15nJe6akzJUYNOw4d
è§£å‹live2d_chat-0.6(gpt3+chatgpt).zipï¼Œ
è¿è¡Œæ¸¸æˆç¨‹åºï¼Œ
å»ºè®®ç”¨renpyä¿®æ”¹æ¸¸æˆç¨‹åºï¼Œå†…ç½®é…ç½®æµç¨‹ï¼Œè‡ªå®šä¹‰ä½ çš„live2dæ¨¡å‹å’Œäº¤äº’æ–¹å¼ã€‚

## How to launch API in your windows or server
(Suggestion) Python == 3.8/3.7
## Clone a VITS repository or iSTFT-VITS repository
```sh
git clone https://github.com/CjangCjengh/vits.git
#git clone https://github.com/innnky/MB-iSTFT-VITS
```
## Adding cleaners&inference_api.py to your project
- Noticing "text_cleaners" in config.json
- Edit 'text'dictionary in the VITS or iSTFT-VITS
- Remove unnecessary imports from text/cleaners.py
- The path of inference_api.py should be like path/to/vits/inference_api.py
- If you want to launch this project in your server, it is recommanded to use iSTFT-VITS for tts: path/to/MB-iSTFT-VITS/inference_api.py
## Install requirements of vits enviornments
```sh
cd vits
#cd MB-iSTFT-VITS
pip install -r requirements.txt
```
## Build Monotonic Alignment Search and run preprocessing
```sh
# Cython-version Monotonoic Alignment Search
cd monotonic_align
mkdir monotonic_align
python setup.py build_ext --inplace
cd vits
#cd MB-iSTFT-VITS
```
## Install requirements for using GPT3/CHATGPT in python
```sh
pip install pydub 
pip install openai
#Not recommended due to demanding requirements
#pip install pyChatGPT
```
## Editing the path of configuration file in inference_api.py
```sh
line26:#è®¾å®šå­˜å‚¨å„ç§æ•°æ®çš„ç›®å½•ï¼Œæ–¹ä¾¿æŸ¥çœ‹ï¼Œé»˜è®¤C:/project_file
line27:current_work_dir = os.path.dirname(__file__)
line28:weight_path = os.path.join(current_work_dir, '/project_file/')
line34:hps_ms = utils.get_hparams_from_file("path/to/config.json")
line43:_ = utils.load_checkpoint("path/to/checkpoint.pth", net_g_ms, None)
```
## For CPU inference in server or those who do not have cuda installed
```sh
#change this line to dev = torch.device("cpu")
line32:dev = torch.device("cuda:0")
```
## launch
```sh
python inference_api.py
```
## What to do next?
As you can see in the temminal
```sh
 * Serving Flask app 'inference_api'
 * Debug mode: on
INFO:werkzeug: WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://10.0.0.14:8080
INFO:werkzeug: Press CTRL+C to quit
```
Which means you can try it in the game now
## Why using api?
ä¸ä¼šçœŸæœ‰äººæƒ³æ¯æ¬¡éƒ½è¦å¯åŠ¨ä¸€å †ç¨‹åºï¼Œé…ç½®ä¸ªåŠå¤©ï¼Œåƒç”µè„‘ä¸€å¤§åŠå†…å­˜å’Œæ˜¾å­˜æ¥è·Ÿçº¸ç‰‡äººèŠå¤©å§ï¼Œåæ­£æˆ‘è°ƒè¯•å®Œä¹‹åè‚¯å®šä¸ä¼šï¼Œ20å—ä¸€ä¸ªæœˆçš„æœåŠ¡å™¨ä¸é¦™å—ï¼Ÿ
## Real usage for api
Building chatroom on my website. Now preparing the live2d models.
## Why not chatgpt?
You can edit it in the inference_api.py
```sh
line143:#CHATGPTæŠ“å–
line144:#session_token = 'å‚è€ƒhttps://www.youtube.com/watch?v=TdNSj_qgdFk'
line145:#api = ChatGPT(session_token)
#Delate the comment in line200 and line233
```
If you want to chat with it indiscriminately as if it is your waifu, the company will stop you lol.
## What to do with game?
Official website of RenPy https://www.renpy.org/
You can follow the instructions and beautify your game, can take my game given as a reference.
